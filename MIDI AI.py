# Import necessary libraries
import reapy  # Library for interacting with REAPER's API
import time  # Used for creating delays and checking periodically
import openai  # Library for using OpenAI's API
from pydantic import BaseModel  # Library for data validation and parsing
from openai import OpenAI
import json  # For working with JSON data
import sys
from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton, QVBoxLayout, QWidget, QFileDialog, QLabel, QTextEdit, QHBoxLayout, QComboBox
from PyQt5.QtGui import QPainter, QColor

# Define a Pydantic model to represent a MIDI note
# This model enforces the structure of a MIDI note object
class MidiNote(BaseModel):
    start: float  # Note start time in seconds
    end: float  # Note end time in seconds
    pitch: int  # MIDI pitch (0-127)
    velocity: int  # Velocity of the note (0-127)
    channel: int  # MIDI channel (0-15)
    selected: bool  # If the note is selected
    muted: bool  # If the note is muted

# Define a Pydantic model to represent the response from OpenAI
# This response will contain a list of orchestrated MIDI notes
class MidiOrchestrationResponse(BaseModel):
    gpt_notes: list[MidiNote]  # List of MIDI notes generated by GPT

# Class to handle MIDI pitch transposition and orchestration
class MidiPitchTransposer:
    def __init__(self):
        self.project = reapy.Project()  # Connect to the current REAPER project

    # Main method to run the MIDI transposition and orchestration
    def run(self):
        # Get the first selected item in the REAPER project
        item = self.project.get_selected_item(0)
        if not item:
            print("No selected item.")  # Print error message if no item is selected
            return

        # Get the active take from the selected item
        take = item.active_take
        if not take:
            print("No active take.")  # Print error message if there is no active take
            return

        # Get all MIDI notes from the active take
        notes = take.notes
        if not notes:
            print("No MIDI notes.")  # Print error message if no MIDI notes are found
            return

        print(f"Found {len(notes)} notes.")  # Print the number of found notes
        note_infos = [note.infos for note in notes]  # Extract information from the notes

        # Send the MIDI notes to ChatGPT for orchestration
        orchestrated_notes = self.send_notes_to_chatgpt(note_infos)

        # Import the orchestrated notes back into REAPER
        self.import_orchestrated_notes(take, orchestrated_notes)

    # Method to send MIDI notes to ChatGPT and get orchestrated notes
    def send_notes_to_chatgpt(self, note_infos, style, key):
        """Send MIDI notes to ChatGPT for orchestration and return the response."""
        
        # Convert the list of note information to a JSON string for ChatGPT
        midi_data = json.dumps(note_infos, default=str)  # Convert to JSON string
        style_prompt = (
            f"Pitches are notes. Orchestrate the notes in the style of {style} in the key of {key}. "
            "Avoid close-voicings and focus on octaves and lush wide voicings. "
            "Do not use duplicate notes or pitches."
        )
        client = OpenAI()  # Initialize the OpenAI client

        # Use ChatGPT to get the orchestrated notes with structured output
        completion = client.beta.chat.completions.parse(
            model="gpt-4o-mini",  # Specify the model to use
            messages=[
                {"role": "system", "content": "You are a master orchestrator who chooses notes and pitches carefully."},
                {"role": "user", "content": f"{style_prompt}:\n{midi_data}"},
            ],
            response_format=MidiOrchestrationResponse,  # Specify the expected schema of the response
        )

        # Extract and return the parsed notes from the structured response
        orchestrated_notes = completion.choices[0].message.parsed
        print(orchestrated_notes)  # Print the orchestrated notes for debugging
        return orchestrated_notes.gpt_notes  # Return the list of orchestrated notes

    # Method to import the orchestrated notes back into the REAPER project
    def import_orchestrated_notes(self, take, orchestrated_notes):
        """Import orchestrated notes back into REAPER."""
        self.project.perform_action(40153)  # Refresh or ensure the context in REAPER
        for note_info in orchestrated_notes:
            # Convert start and end times from seconds to PPQ (Pulses Per Quarter note)
            start_ppq = take.time_to_ppq(note_info.start)
            end_ppq = take.time_to_ppq(note_info.end)

            # Add the orchestrated note to the active take in REAPER
            take.add_note(
                start=start_ppq,
                end=end_ppq,
                pitch=note_info.pitch,
                velocity=note_info.velocity,
                channel=note_info.channel,
                selected=note_info.selected,
                muted=note_info.muted,
                unit="ppq",  # Specify that we are using PPQ units
                sort=True  # Ensure notes are sorted correctly
            )

# Function to wait for REAPER to stop recording, then run the MIDI transposer
def wait_for_stop_recording():
    project = reapy.Project()  # Connect to the current REAPER project
    was_recording = project.is_recording  # Check if the project is recording
    midi_transposer = MidiPitchTransposer()  # Create an instance of the transposer

    # Loop to check if recording has stopped
    while True:
        is_recording = project.is_recording  # Check if the project is currently recording
        if was_recording and not is_recording:  # If recording was active and has now stopped
            print("Recording stopped.")  # Print message
            midi_transposer.run()  # Run the transposer
        was_recording = is_recording  # Update recording status
        time.sleep(0.5)  # Wait for half a second before checking again

# Entry point of the script
if __name__ == "__main__":
    wait_for_stop_recording()  # Start monitoring for when recording stops

class MidiAssistantApp(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Advanced AI MIDI Assistant")
        self.setGeometry(100, 100, 800, 600)
        self.main_widget = QWidget(self)
        self.setCentralWidget(self.main_widget)
        self.layout = QVBoxLayout(self.main_widget)
        self.add_ui_elements()
        self.midi_transposer = MidiPitchTransposer()
        self.midi_notes = []

    def add_ui_elements(self):
        self.load_button = QPushButton("Load MIDI File", self)
        self.load_button.clicked.connect(self.load_midi_file)
        self.layout.addWidget(self.load_button)

        self.style_combo = QComboBox(self)
        self.style_combo.addItems(["Bach", "Beethoven", "Mozart", "Chopin", "Debussy"])
        self.layout.addWidget(QLabel("Select Composer Style:"))
        self.layout.addWidget(self.style_combo)

        self.key_combo = QComboBox(self)
        self.key_combo.addItems(["C Major", "G Major", "D Major", "A Minor", "E Minor"])
        self.layout.addWidget(QLabel("Select Key:"))
        self.layout.addWidget(self.key_combo)

        self.send_button = QPushButton("Send to AI", self)
        self.send_button.clicked.connect(self.send_to_ai)
        self.layout.addWidget(self.send_button)

        self.save_button = QPushButton("Save Orchestrated MIDI", self)
        self.save_button.clicked.connect(self.save_midi_file)
        self.layout.addWidget(self.save_button)

        self.result_area = QTextEdit(self)
        self.result_area.setReadOnly(True)
        self.layout.addWidget(self.result_area)

        self.status_label = QLabel("Status: Ready", self)
        self.layout.addWidget(self.status_label)

    def load_midi_file(self):
        options = QFileDialog.Options()
        file_name, _ = QFileDialog.getOpenFileName(self, "Open MIDI File", "", "MIDI Files (*.mid);;All Files (*)", options=options)
        if file_name:
            self.status_label.setText(f"Loaded: {file_name}")
            self.midi_transposer.run()

    def send_to_ai(self):
        self.status_label.setText("Sending to AI...")
        selected_style = self.style_combo.currentText()
        selected_key = self.key_combo.currentText()
        orchestrated_notes = self.midi_transposer.send_notes_to_chatgpt(self.midi_notes, selected_style, selected_key)
        self.result_area.setText(str(orchestrated_notes))
        self.status_label.setText("AI Orchestration Complete")

    def save_midi_file(self):
        options = QFileDialog.Options()
        file_name, _ = QFileDialog.getSaveFileName(self, "Save Orchestrated MIDI", "", "MIDI Files (*.mid);;All Files (*)", options=options)
        if file_name:
            # Logic to save the orchestrated MIDI notes to a file
            self.status_label.setText(f"Saved: {file_name}")

    def paintEvent(self, event):
        painter = QPainter(self)
        painter.setPen(QColor(0, 0, 0))
        # Example visualization logic
        for note in self.midi_notes:
            painter.drawRect(note.start * 10, note.pitch, 10, 10)

def main():
    app = QApplication(sys.argv)
    window = MidiAssistantApp()
    window.show()
    sys.exit(app.exec_())

if __name__ == "__main__":
    main()
