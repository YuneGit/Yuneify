# Import necessary libraries
import reapy  # Library for interacting with REAPER's API
import time  # Used for creating delays and checking periodically
import openai  # Library for using OpenAI's API
from pydantic import BaseModel  # Library for data validation and parsing
from openai import OpenAI
import json  # For working with JSON data

# Define a Pydantic model to represent a MIDI note
# This model enforces the structure of a MIDI note object
class MidiNote(BaseModel):
    start: float  # Note start time in seconds
    end: float  # Note end time in seconds
    pitch: int  # MIDI pitch (0-127)
    velocity: int  # Velocity of the note (0-127)
    channel: int  # MIDI channel (0-15)
    selected: bool  # If the note is selected
    muted: bool  # If the note is muted

# Define a Pydantic model to represent the response from OpenAI
# This response will contain a list of orchestrated MIDI notes
class MidiOrchestrationResponse(BaseModel):
    gpt_notes: list[MidiNote]  # List of MIDI notes generated by GPT

# Class to handle MIDI pitch transposition and orchestration
class MidiPitchTransposer:
    def __init__(self):
        self.project = reapy.Project()  # Connect to the current REAPER project

    # Main method to run the MIDI transposition and orchestration
    def run(self):
        # Get the first selected item in the REAPER project
        item = self.project.get_selected_item(0)
        if not item:
            print("No selected item.")  # Print error message if no item is selected
            return

        # Get the active take from the selected item
        take = item.active_take
        if not take:
            print("No active take.")  # Print error message if there is no active take
            return

        # Get all MIDI notes from the active take
        notes = take.notes
        if not notes:
            print("No MIDI notes.")  # Print error message if no MIDI notes are found
            return

        print(f"Found {len(notes)} notes.")  # Print the number of found notes
        note_infos = [note.infos for note in notes]  # Extract information from the notes

        # Send the MIDI notes to ChatGPT for orchestration
        orchestrated_notes = self.send_notes_to_chatgpt(note_infos)

        # Import the orchestrated notes back into REAPER
        self.import_orchestrated_notes(take, orchestrated_notes)

    # Method to send MIDI notes to ChatGPT and get orchestrated notes
    def send_notes_to_chatgpt(self, note_infos):
        """Send MIDI notes to ChatGPT for orchestration and return the response."""
        
        # Convert the list of note information to a JSON string for ChatGPT
        midi_data = json.dumps(note_infos, default=str)  # Convert to JSON string
        style = (
            "Pitches are notes. Orchestrate the notes. Avoid close-voicings and "
            "focus on octaves and lush wide voicings. Determine the key based on the input. "
            "Do not use duplicate notes or pitches."
        )
        client = OpenAI()  # Initialize the OpenAI client

        # Use ChatGPT to get the orchestrated notes with structured output
        completion = client.beta.chat.completions.parse(
            model="gpt-4o-mini",  # Specify the model to use
            messages=[
                {"role": "system", "content": "You are a master orchestrator who chooses notes and pitches carefully."},
                {"role": "user", "content": f"{style}:\n{midi_data}"},
            ],
            response_format=MidiOrchestrationResponse,  # Specify the expected schema of the response
        )

        # Extract and return the parsed notes from the structured response
        orchestrated_notes = completion.choices[0].message.parsed
        print(orchestrated_notes)  # Print the orchestrated notes for debugging
        return orchestrated_notes.gpt_notes  # Return the list of orchestrated notes

    # Method to import the orchestrated notes back into the REAPER project
    def import_orchestrated_notes(self, take, orchestrated_notes):
        """Import orchestrated notes back into REAPER."""
        self.project.perform_action(40153)  # Refresh or ensure the context in REAPER
        for note_info in orchestrated_notes:
            # Convert start and end times from seconds to PPQ (Pulses Per Quarter note)
            start_ppq = take.time_to_ppq(note_info.start)
            end_ppq = take.time_to_ppq(note_info.end)

            # Add the orchestrated note to the active take in REAPER
            take.add_note(
                start=start_ppq,
                end=end_ppq,
                pitch=note_info.pitch,
                velocity=note_info.velocity,
                channel=note_info.channel,
                selected=note_info.selected,
                muted=note_info.muted,
                unit="ppq",  # Specify that we are using PPQ units
                sort=True  # Ensure notes are sorted correctly
            )

# Function to wait for REAPER to stop recording, then run the MIDI transposer
def wait_for_stop_recording():
    project = reapy.Project()  # Connect to the current REAPER project
    was_recording = project.is_recording  # Check if the project is recording
    midi_transposer = MidiPitchTransposer()  # Create an instance of the transposer

    # Loop to check if recording has stopped
    while True:
        is_recording = project.is_recording  # Check if the project is currently recording
        if was_recording and not is_recording:  # If recording was active and has now stopped
            print("Recording stopped.")  # Print message
            midi_transposer.run()  # Run the transposer
        was_recording = is_recording  # Update recording status
        time.sleep(0.5)  # Wait for half a second before checking again

# Entry point of the script
if __name__ == "__main__":
    wait_for_stop_recording()  # Start monitoring for when recording stops
