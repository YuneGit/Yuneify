# Import necessary libraries
import reapy  # Library for interacting with REAPER's API
import time  # Used for creating delays and checking periodically
import openai  # Library for using OpenAI's API
from pydantic import BaseModel  # Library for data validation and parsing
from openai import OpenAI
import json  # For working with JSON data
import sys  # For system-specific parameters and functions
from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QLabel, QPushButton, QComboBox, QLineEdit
from PyQt5.QtCore import QTimer

# Define a Pydantic model to represent a MIDI note
# This model enforces the structure of a MIDI note object
class MidiNote(BaseModel):
    start: float  # Note start time in seconds
    end: float  # Note end time in seconds
    pitch: int  # MIDI pitch (0-127)
    velocity: int  # Velocity of the note (0-127)
    channel: int  # MIDI channel (0-15)
    selected: bool  # If the note is selected
    muted: bool  # If the note is muted

# Define a Pydantic model to represent the response from OpenAI
# This response will contain a list of orchestrated MIDI notes
class MidiOrchestrationResponse(BaseModel):
    gpt_notes: list[MidiNote]  # List of MIDI notes generated by GPT

class AIOrchestrationStyleSelector(QWidget):
    def __init__(self):
        super().__init__()
        self.initUI()
        self.midi_orchestrator = AIMidiOrchestrator()  # Initialize orchestrator
        self.project = reapy.Project()
        self.was_recording = self.project.is_recording

        # Set up a timer to check the recording status
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.check_recording_status)
        self.timer.start(500)  # Check every 500 milliseconds

    def initUI(self):
        self.setWindowTitle('Select Orchestration Style')
        layout = QVBoxLayout()

        # Label
        label = QLabel('Choose an orchestration style:')
        layout.addWidget(label)

        # ComboBox for style selection
        self.style_combo = QComboBox(self)
        self.style_combo.addItems([
            'Default',
            'Composers: Bach', 'Composers: Mozart', 'Composers: Beethoven', 'Composers: Debussy',
            'Genres: Jazz', 'Genres: Rock', 'Genres: Electronic', 'Genres: Classical',
            'Characteristics: Lush', 'Characteristics: Sparse', 'Characteristics: Rhythmic', 'Characteristics: Melodic'
        ])
        layout.addWidget(self.style_combo)

        # Text input for additional prompt instructions
        self.prompt_input = QLineEdit(self)
        self.prompt_input.setPlaceholderText('Add custom instructions for orchestration...')
        layout.addWidget(self.prompt_input)

        # Button to confirm selection
        self.select_button = QPushButton('Select', self)
        self.select_button.clicked.connect(self.on_select)
        layout.addWidget(self.select_button)

        # Feedback label
        self.feedback_label = QLabel('', self)
        layout.addWidget(self.feedback_label)

        self.setLayout(layout)

    def on_select(self):
        selected_style = self.style_combo.currentText()
        custom_instructions = self.prompt_input.text()
        print(f"Selected style: {selected_style}")
        self.feedback_label.setText(f"Style '{selected_style}' selected. Processing...")

        # Update the orchestrator with the selected style and custom instructions
        self.midi_orchestrator.style = selected_style
        self.midi_orchestrator.custom_instructions = custom_instructions
        self.midi_orchestrator.run()

    def check_recording_status(self):
        is_recording = self.project.is_recording
        if self.was_recording and not is_recording:
            print("Recording stopped.")
            self.on_select()  # Trigger orchestration
        self.was_recording = is_recording

    def get_selected_style(self):
        return self.style_combo.currentText()

# Class to handle MIDI pitch transposition and orchestration
class AIMidiOrchestrator:
    def __init__(self, style='Default', custom_instructions=''):
        self.project = reapy.Project()  # Connect to the current REAPER project
        self.style = style
        self.custom_instructions = custom_instructions

    # Main method to run the MIDI transposition and orchestration
    def run(self):
        # Get the first selected item in the REAPER project
        item = self.project.get_selected_item(0)
        if not item:
            print("No selected item.")  # Print error message if no item is selected
            return

        # Get the active take from the selected item
        take = item.active_take
        if not take:
            print("No active take.")  # Print error message if there is no active take
            return

        # Get all MIDI notes from the active take
        notes = take.notes
        if not notes:
            print("No MIDI notes.")  # Print error message if no MIDI notes are found
            return

        print(f"Found {len(notes)} notes.")  # Print the number of found notes
        note_infos = [note.infos for note in notes]  # Extract information from the notes

        # Send the MIDI notes to ChatGPT for orchestration
        orchestrated_notes = self.send_notes_to_chatgpt(note_infos)

        # Import the orchestrated notes back into REAPER
        self.import_orchestrated_notes(take, orchestrated_notes)

    # Method to send MIDI notes to ChatGPT and get orchestrated notes
    def send_notes_to_chatgpt(self, note_infos):
        """Send MIDI notes to ChatGPT for orchestration and return the response."""
        
        # Convert the list of note information to a JSON string for ChatGPT
        midi_data = json.dumps(note_infos, default=str)  # Convert to JSON string
        style = (
            f"Pitches are notes. Orchestrate the notes in the style of {self.style}. "
            f"{self.custom_instructions} "
            "Avoid close-voicings and focus on octaves and lush wide voicings. "
            "Determine the key based on the input. Do not use duplicate notes or pitches."
        )
        client = OpenAI()  # Initialize the OpenAI client

        # Use ChatGPT to get the orchestrated notes with structured output
        completion = client.beta.chat.completions.parse(
            model="gpt-4o-mini",  # Specify the model to use
            messages=[
                {"role": "system", "content": "You are a master orchestrator who chooses notes and pitches carefully."},
                {"role": "user", "content": f"{style}:\n{midi_data}"},
            ],
            response_format=MidiOrchestrationResponse,  # Specify the expected schema of the response
        )

        # Extract and return the parsed notes from the structured response
        orchestrated_notes = completion.choices[0].message.parsed
        print(orchestrated_notes)  # Print the orchestrated notes for debugging
        return orchestrated_notes.gpt_notes  # Return the list of orchestrated notes

    # Method to import the orchestrated notes back into the REAPER project
    def import_orchestrated_notes(self, take, orchestrated_notes):
        """Import orchestrated notes back into REAPER."""
        self.project.perform_action(40153)  # Refresh or ensure the context in REAPER
        for note_info in orchestrated_notes:
            # Convert start and end times from seconds to PPQ (Pulses Per Quarter note)
            start_ppq = take.time_to_ppq(note_info.start)
            end_ppq = take.time_to_ppq(note_info.end)

            # Add the orchestrated note to the active take in REAPER
            take.add_note(
                start=start_ppq,
                end=end_ppq,
                pitch=note_info.pitch,
                velocity=note_info.velocity,
                channel=note_info.channel,
                selected=note_info.selected,
                muted=note_info.muted,
                unit="ppq",  # Specify that we are using PPQ units
                sort=True  # Ensure notes are sorted correctly
            )

# Entry point of the script
if __name__ == "__main__":
    app = QApplication(sys.argv)
    style_selector = AIOrchestrationStyleSelector()
    style_selector.show()
    sys.exit(app.exec_())
