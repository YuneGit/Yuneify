# Import necessary libraries
import reapy  # Library for interacting with REAPER's API
import time  # Used for creating delays and checking periodically
import openai  # Library for using OpenAI's API
from pydantic import BaseModel  # Library for data validation and parsing
from openai import OpenAI
import json  # For working with JSON data
import sys  # For system-specific parameters and functions
from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QLabel, QPushButton, QComboBox, QLineEdit
from PyQt5.QtCore import QTimer

# Define a Pydantic model to represent a MIDI note
# This model enforces the structure of a MIDI note object
class MidiNote(BaseModel):
    start: float  # Note start time in seconds
    end: float  # Note end time in seconds
    pitch: int  # MIDI pitch (0-127)
    velocity: int  # Velocity of the note (0-127)
    channel: int  # MIDI channel (0-15)
    selected: bool  # If the note is selected
    muted: bool  # If the note is muted

# Define a Pydantic model to represent the response from OpenAI
# This response will contain a list of orchestrated MIDI notes
class MidiOrchestrationResponse(BaseModel):
    gpt_notes: list[MidiNote]  # List of MIDI notes generated by GPT

class AIOrchestrationStyleSelector(QWidget):
    def __init__(self):
        super().__init__()
        self.initUI()
        self.midi_orchestrator = AIMidiOrchestrator()  # Initialize orchestrator

    def initUI(self):
        self.setWindowTitle('Select Orchestration Style')
        self.setFixedSize(275, 175)  # Set fixed size to 275x150
        self.setFixedWidth(275)

        # Apply modern dark mode style
        self.setStyleSheet("""
            QWidget {
                background-color: #1E1E1E;
                color: #FFFFFF;
                border-radius: 10px;
            }
            QLabel {
                color: #FFFFFF;
                font-size: 16px;
                margin: 5px;
            }
            QPushButton {
                background-color: #3A3A3A;
                color: #FFFFFF;
                border-radius: 12px;
                padding: 4px 4px;
                font-size: 14px;
                border: 1px solid #5A5A5A;
            }
            QPushButton:hover {
                background-color: #4A4A4A;
            }
            QPushButton:pressed {
                background-color: #2A2A2A;
            }
            QComboBox {
                background-color: #3A3A3A;
                color: #FFFFFF;
                border-radius: 3px;
                padding: 3px;
                border: 1px solid #5A5A5A;
            }
            QComboBox QAbstractItemView {
                background-color: #2A2A2A;
                color: #FFFFFF;
                selection-background-color: #4A4A4A;
            }
            QLineEdit {
                background-color: #2A2A2A;
                color: #FFFFFF;
                border-radius: 3px;
                padding: 3px;
                border: 1px solid #5A5A5A;
            }
        """)

        layout = QVBoxLayout()

        # Label
        label = QLabel('Choose an orchestration style:')
        layout.addWidget(label)

        # ComboBox for style selection
        self.style_combo = QComboBox(self)
        self.style_combo.addItems([
            'Default',
            'Composers: Bach', 'Composers: Mozart', 'Composers: Beethoven', 'Composers: Debussy',
            'Genres: Jazz', 'Genres: Rock', 'Genres: Electronic', 'Genres: Classical',
            'Characteristics: Lush', 'Characteristics: Sparse', 'Characteristics: Rhythmic', 'Characteristics: Melodic'
        ])
        layout.addWidget(self.style_combo)

        # Text input for additional prompt instructions
        self.prompt_input = QLineEdit(self)
        self.prompt_input.setPlaceholderText('Add custom instructions for orchestration...')
        layout.addWidget(self.prompt_input)

        # Button to send notes to GPT
        self.send_button = QPushButton('Send to GPT', self)
        self.send_button.clicked.connect(self.on_send_to_gpt)
        layout.addWidget(self.send_button)

        # Feedback label
        self.feedback_label = QLabel('', self)
        layout.addWidget(self.feedback_label)

        self.setLayout(layout)

    def on_send_to_gpt(self):
        selected_style = self.style_combo.currentText()
        custom_instructions = self.prompt_input.text()
        print(f"Selected style: {selected_style}")
        self.feedback_label.setText(f"Style '{selected_style}' selected. Processing...")

        # Update the orchestrator with the selected style and custom instructions
        self.midi_orchestrator.style = selected_style
        self.midi_orchestrator.custom_instructions = custom_instructions
        self.midi_orchestrator.run()

    def get_selected_style(self):
        return self.style_combo.currentText()

# Class to handle MIDI pitch transposition and orchestration
class AIMidiOrchestrator:
    def __init__(self, style='Default', custom_instructions=''):
        with reapy.inside_reaper():
            self.project = reapy.Project()  # Connect to the current REAPER project
        self.style = style
        self.custom_instructions = custom_instructions

    # Main method to run the MIDI transposition and orchestration
    def run(self):
        with reapy.inside_reaper():
            item = self.project.get_selected_item(0)
            if not item:
                print("No selected item.")
                return

            take = item.active_take
            if not take:
                print("No active take.")
                return

            notes = take.notes
            if not notes:
                print("No MIDI notes.")
                return

            print(f"Found {len(notes)} notes.")
            note_infos = [note.infos for note in notes]

            orchestrated_notes = self.send_notes_to_chatgpt(note_infos)
            self.import_orchestrated_notes(take, orchestrated_notes)

    # Method to send MIDI notes to ChatGPT and get orchestrated notes
    def send_notes_to_chatgpt(self, note_infos):
        """Send MIDI notes to ChatGPT for orchestration and return the response."""
        
        # Convert the list of note information to a JSON string for ChatGPT
        midi_data = json.dumps(note_infos, default=str)  # Convert to JSON string
        style = (
            f"Pitches are notes. Orchestrate the notes in the style of {self.style}. "
            f"{self.custom_instructions} "
            "Avoid close-voicings and focus on creating three note chords and lush wide voicings. "
            "Determine the key based on the input. Do not use duplicate notes or pitches."
        )
        client = OpenAI()  # Initialize the OpenAI client

        # Use ChatGPT to get the orchestrated notes with structured output
        completion = client.beta.chat.completions.parse(
            model="gpt-4o-mini",  # Specify the model to use
            messages=[
                {"role": "system", "content": "You are a master orchestrator who chooses notes and pitches carefully."},
                {"role": "user", "content": f"{style}:\n{midi_data}"},
            ],
            response_format=MidiOrchestrationResponse,  # Specify the expected schema of the response
        )

        # Extract and return the parsed notes from the structured response
        orchestrated_notes = completion.choices[0].message.parsed
        print(orchestrated_notes)  # Print the orchestrated notes for debugging
        return orchestrated_notes.gpt_notes  # Return the list of orchestrated notes

    # Method to import the orchestrated notes back into the REAPER project
    def import_orchestrated_notes(self, take, orchestrated_notes):
        """Import orchestrated notes back into REAPER."""
        with reapy.inside_reaper():
            self.project.perform_action(40153)
            for note_info in orchestrated_notes:
                start_ppq = take.time_to_ppq(note_info.start)
                end_ppq = take.time_to_ppq(note_info.end)
                take.add_note(
                    start=start_ppq,
                    end=end_ppq,
                    pitch=note_info.pitch,
                    velocity=note_info.velocity,
                    channel=note_info.channel,
                    selected=note_info.selected,
                    muted=note_info.muted,
                    unit="ppq",
                    sort=True
                )

# Entry point of the script
if __name__ == "__main__":
    app = QApplication(sys.argv)
    style_selector = AIOrchestrationStyleSelector()
    style_selector.show()
    sys.exit(app.exec_())
